<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
    
    <article class="container">
        <section class="intro flow">
          <h2>ReLu in Machine Learning</h2>
          <p>In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the non-negative part of its argument </p>
          <button onclick="window.location.href='http://itsmyspace.xyz/robot_dimension/main.html';">Go back</button>
        </section>
        <section class="details">
          <header>Header</header>
          <div class="scroll-container">
            Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid, which was trained in a supervised way to learn several computer vision tasks. In 2011, the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units, compared to sigmoid function or similar activation functions, allow faster and effective training of deep neural architectures on large and complex datasets.
          </div>
          <footer>Footer</footer>
        </section>
      </article>


      <link rel="stylesheet" href="detail.css">
</body>
</html>
